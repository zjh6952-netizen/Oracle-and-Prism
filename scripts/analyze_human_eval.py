import pandas as pd
import numpy as np
from scipy import stats
import os
import re

# --- é…ç½® ---
PROJECT_ROOT = "/root/autodl-tmp/GenRec_Explainer_Project"
# ä½ ä»é—®å·ç½‘ç«™å¯¼å‡ºçš„åŸå§‹æ‰“åˆ†æ•°æ® (å¤šä½è¯„ä¼°è€…çš„ç­”å·)
RAW_RESULTS_FILE = os.path.join(PROJECT_ROOT, "results", "raw_survey_results_exported_from_wjx.csv")
# æˆ‘ä»¬ä¹‹å‰ç”Ÿæˆçš„â€œç­”æ¡ˆå¯†é’¥â€ (è®°å½•äº†A/B/Cå¯¹åº”å“ªä¸ªæ¨¡å‹)
KEY_FILE = os.path.join(PROJECT_ROOT, "results", "human_evaluation_survey_data_WITH_IDS.csv")
# æœ€ç»ˆç»Ÿè®¡ç»“æœçš„è¾“å‡ºè·¯å¾„
STATS_OUTPUT_FILE = os.path.join(PROJECT_ROOT, "results", "final_human_eval_stats.csv")
PLOT_OUTPUT_FILE = os.path.join(PROJECT_ROOT, "results", "human_eval_plot.png") # æ–°å¢ï¼šå›¾è¡¨è¾“å‡º

# --- ä¸»é€»è¾‘ ---
print("--- å¼€å§‹ç»Ÿè®¡äººå·¥è¯„ä¼°ç»“æœ ---")

# 1. åŠ è½½æ•°æ®
print("1. æ­£åœ¨åŠ è½½æ•°æ®...")
raw_df = pd.read_csv(RAW_RESULTS_FILE)
key_df = pd.read_csv(KEY_FILE)

# æ‰“å°åŸå§‹æ•°æ®çš„åˆ—åï¼Œæ–¹ä¾¿è°ƒè¯•
print(f"  åŸå§‹æ•°æ®å…±æœ‰ {len(raw_df)} è¡Œï¼ˆè¯„ä¼°è€…ç­”å·æ•°ï¼‰")
print(f"  åŸå§‹æ•°æ®åˆ—åç¤ºä¾‹: {list(raw_df.columns)[:5]}...") 

# 2. æ•°æ®æ•´ç†ï¼šå°†â€œå®½è¡¨â€å˜æˆâ€œé•¿è¡¨â€
print("2. æ­£åœ¨é‡å¡‘æ•°æ®ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´ï¼‰...")

# åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨æ‰€æœ‰æ‰“åˆ†è®°å½•
all_scores = []

# éå†â€œç­”æ¡ˆå¯†é’¥â€ä¸­çš„æ¯ä¸€ä¸ªåœºæ™¯ï¼ˆæ ·æœ¬ï¼‰
for scene_index, scene_row in key_df.iterrows():
    sample_id = scene_row['sample_id']
    
    # éå†åŸå§‹æ•°æ®ä¸­çš„æ¯ä¸€ä½è¯„ä¼°è€…
    for evaluator_index, evaluator_row in raw_df.iterrows():
        # æ„å»ºæˆ‘ä»¬è¦åœ¨åŸå§‹æ•°æ®ä¸­æŸ¥æ‰¾çš„åˆ—åæ¨¡å¼
        # ä¾‹å¦‚ï¼šæŸ¥æ‰¾åŒ…å«â€œåœºæ™¯1â€ã€â€œè§£é‡ŠAâ€ã€â€œè¯´æœåŠ›â€çš„åˆ—
        base_pattern = f"åœºæ™¯{sample_id+1}|åœºæ™¯{scene_index+1}" # é—®å·æ˜Ÿå¯èƒ½ä»åœºæ™¯1æˆ–ç´¢å¼•+1å¼€å§‹
        
        for expl_char in ['A', 'B', 'C']:
            # ä»â€œå¯†é’¥â€ä¸­è·å–å½“å‰è§£é‡Šå­—ç¬¦å¯¹åº”çš„æ¨¡å‹
            target_model = scene_row[f'explanation_{expl_char}_model']
            target_text = scene_row[f'explanation_{expl_char}_text']
            
            # åœ¨åŸå§‹æ•°æ®ä¸­æŸ¥æ‰¾åŒ¹é…çš„åˆ—
            for metric in ['è¯´æœåŠ›', 'ä¸ªæ€§åŒ–', 'å¿ å®åº¦']:
                # æ„å»ºä¸€ä¸ªæ›´çµæ´»çš„æ¨¡å¼ï¼šåŒ¹é…åŒ…å«ï¼ˆåœºæ™¯IDã€è§£é‡Šå­—ç¬¦ã€ç»´åº¦ï¼‰çš„åˆ—
                pattern = f".*{base_pattern}.*è§£é‡Š{expl_char}.*{metric}.*"
                
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾åŒ¹é…çš„åˆ—
                matching_columns = [col for col in raw_df.columns if re.search(pattern, col, re.IGNORECASE)]
                
                if not matching_columns:
                    print(f"  è­¦å‘Š: æœªæ‰¾åˆ°åŒ¹é…çš„åˆ—: {pattern}")
                    continue
                if len(matching_columns) > 1:
                    print(f"  è­¦å‘Š: æ‰¾åˆ°å¤šä¸ªåŒ¹é…åˆ— {matching_columns}ï¼Œå°†ä½¿ç”¨ç¬¬ä¸€ä¸ª")
                
                target_column = matching_columns[0]
                score_value = evaluator_row[target_column]
                
                # å°†è¿™æ¡æ‰“åˆ†è®°å½•æ·»åŠ åˆ°åˆ—è¡¨ä¸­
                all_scores.append({
                    'sample_id': sample_id,
                    'evaluator_id': evaluator_index,
                    'model': target_model,
                    'metric': metric,
                    'score': score_value,
                    'explanation_text': target_text # å¯é€‰ï¼Œç”¨äºè°ƒè¯•
                })

# å°†åˆ—è¡¨è½¬æ¢ä¸ºDataFrame
scores_df = pd.DataFrame(all_scores)
print(f"  æˆåŠŸæ•´ç†å‡º {len(scores_df)} æ¡æ‰“åˆ†è®°å½•ã€‚")

# 3. è®¡ç®—å¹³å‡åˆ†å’Œæ ‡å‡†å·®
print("3. æ­£åœ¨è®¡ç®—æè¿°æ€§ç»Ÿè®¡é‡...")
# æŒ‰æ¨¡å‹å’Œç»´åº¦åˆ†ç»„è®¡ç®—
stats_df = scores_df.groupby(['model', 'metric']).agg(
    mean_score=('score', 'mean'),
    std_score=('score', 'std'),
    count=('score', 'count')
).reset_index()

print("\n--- æœ€ç»ˆç»Ÿè®¡ç»“æœ (å¹³å‡åˆ† Â± æ ‡å‡†å·®) ---")
for model in ['GenRec-E', 'FLAN-T5-XXL', 'BART-Base']:
    print(f"\n{model}:")
    model_data = stats_df[stats_df['model'] == model]
    for _, row in model_data.iterrows():
        print(f"  {row['metric']}: {row['mean_score']:.3f} Â± {row['std_score']:.3f}")

# 4. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ (é…å¯¹Tæ£€éªŒ)
print("\n4. æ­£åœ¨è¿›è¡Œæ˜¾è‘—æ€§æ£€éªŒ (Paired T-Test)...")
# æˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªæ ·æœ¬ã€æ¯ä¸ªè¯„ä¼°è€…ã€æ¯ä¸ªç»´åº¦ï¼Œæ„å»ºä¸€ä¸ªåŒ…å«ä¸‰ä¸ªæ¨¡å‹åˆ†æ•°çš„å®½è¡¨
pivot_df = scores_df.pivot_table(
    index=['sample_id', 'evaluator_id', 'metric'],
    columns='model',
    values='score'
).reset_index()

# åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨p-valueç»“æœ
p_values = {}

# å¯¹æ¯” GenRec-E ä¸æ¯ä¸ªåŸºçº¿æ¨¡å‹ï¼Œåœ¨æ¯ä¸ªç»´åº¦ä¸Š
baselines = ['FLAN-T5-XXL', 'BART-Base']
metrics = ['è¯´æœåŠ›', 'ä¸ªæ€§åŒ–', 'å¿ å®åº¦']

for baseline in baselines:
    for metric in metrics:
        # æå–å½“å‰ç»´åº¦çš„æ•°æ®
        data_genrec = pivot_df[pivot_df['metric'] == metric]['GenRec-E'].dropna()
        data_baseline = pivot_df[pivot_df['metric'] == metric][baseline].dropna()
        
        # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´ï¼ˆåŸºäºç›¸åŒçš„æ ·æœ¬å’Œè¯„ä¼°è€…ï¼‰
        min_len = min(len(data_genrec), len(data_baseline))
        data_genrec = data_genrec.iloc[:min_len]
        data_baseline = data_baseline.iloc[:min_len]
        
        # æ‰§è¡Œé…å¯¹Tæ£€éªŒ
        t_stat, p_val = stats.ttest_rel(data_genrec, data_baseline)
        p_values[f'GenRec-E_vs_{baseline}_{metric}'] = p_val
        significance = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*" if p_val < 0.05 else "n.s."
        print(f"GenRec-E vs {baseline} ({metric}): p = {p_val:.5f} {significance}")

# 5. ä¿å­˜ç»“æœ
print("5. æ­£åœ¨ä¿å­˜ç»“æœ...")
# ä¿å­˜ç»Ÿè®¡ç»“æœ
stats_df.to_csv(STATS_OUTPUT_FILE, index=False, encoding='utf-8-sig')

# ä¿å­˜p-valueç»“æœ
p_df = pd.DataFrame(list(p_values.items()), columns=['comparison', 'p_value'])
p_df.to_csv(STATS_OUTPUT_FILE.replace('.csv', '_pvalues.csv'), index=False, encoding='utf-8-sig')

print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
print(f"  æè¿°æ€§ç»Ÿè®¡ç»“æœå·²ä¿å­˜è‡³: {STATS_OUTPUT_FILE}")
print(f"  æ˜¾è‘—æ€§æ£€éªŒç»“æœå·²ä¿å­˜è‡³: {STATS_OUTPUT_FILE.replace('.csv', '_pvalues.csv')}")