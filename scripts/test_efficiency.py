import os
import torch
import time
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration, T5Tokenizer

# ==============================================================================
# 1. é…ç½®éƒ¨åˆ† (CONFIGURATION)
# ==============================================================================

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
PROJECT_ROOT = "/root/autodl-tmp/GenRec_Explainer_Project"

# --- æ¨¡å‹è·¯å¾„ ---
# !! å…³é”® !!: è¯·ç¡®ä¿è¿™é‡Œçš„ 'YYYYMMDD_HHMMSS' æ˜¯ä½ çœŸå®çš„è®­ç»ƒè¾“å‡ºæ–‡ä»¶å¤¹çš„æ—¶é—´æˆ³
YOUR_MODEL_WEIGHTS_PATH = os.path.join(PROJECT_ROOT, "results", "20250918_165141", "best_model.mdl") 
YOUR_MODEL_BASE_PATH = os.path.join(PROJECT_ROOT, "models", "bart-base", "facebook", "bart-base")
BASELINE_MODEL_PATH = os.path.join(PROJECT_ROOT, "models", "flan-t5-xxl")

# æˆ‘ä»¬åªéœ€è¦ä¸€æ¡æ ·ä¾‹è¾“å…¥æ¥è¿›è¡Œé€Ÿåº¦æµ‹è¯•
DUMMY_HISTORY = "Inception, Interstellar, The Dark Knight"
DUMMY_ITEM = "The Prestige"
DUMMY_PROMPT = f"Instruction: Generate a personalized explanation for the given recommendation.\nInput: User History: {DUMMY_HISTORY}\nRecommended Item: {DUMMY_ITEM}"

# æµ‹è¯•å‚æ•°
WARMUP_RUNS = 10      # é¢„çƒ­æ¬¡æ•°
TIMING_RUNS = 100     # æ­£å¼è®¡æ—¶æ¬¡æ•°

# ==============================================================================
# 2. æ ¸å¿ƒæµ‹è¯•å‡½æ•°
# ==============================================================================

def benchmark_model(model_name, model, tokenizer):
    """
    ä¸€ä¸ªé€šç”¨çš„å‡½æ•°ï¼Œç”¨äºåŠ è½½æ¨¡å‹ã€æµ‹é‡æ˜¾å­˜å¹¶æµ‹è¯•å…¶ç”Ÿæˆå»¶è¿Ÿã€‚
    """
    print(f"\n--- æ­£åœ¨æµ‹è¯•æ¨¡å‹: {model_name} ---")
    
    # --- 1. æµ‹é‡æ˜¾å­˜å ç”¨ ---
    # PyTorchçš„æ˜¾å­˜ç®¡ç†æ¯”è¾ƒå¤æ‚ï¼Œæˆ‘ä»¬æµ‹é‡å·²åˆ†é…çš„æ˜¾å­˜
    torch.cuda.empty_cache() # æ¸…ç†ä¸€ä¸‹ç¼“å­˜
    initial_memory = torch.cuda.memory_allocated(DEVICE)
    # å®é™…ä¸Šæ¨¡å‹å·²ç»åœ¨åŠ è½½æ—¶è¢«åˆ†é…äº†æ˜¾å­˜ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸»è¦çœ‹å³°å€¼
    print(f"æ¨¡å‹åŠ è½½åï¼Œå·²åˆ†é…æ˜¾å­˜: {initial_memory / 1024**3:.2f} GB")
    
    # --- 2. ç¼–ç è¾“å…¥ ---
    inputs = tokenizer(DUMMY_PROMPT, return_tensors="pt").to(DEVICE)

    # --- 3. GPUé¢„çƒ­ (Warmup) ---
    print(f"æ­£åœ¨è¿›è¡Œ {WARMUP_RUNS} æ¬¡é¢„çƒ­...")
    for _ in range(WARMUP_RUNS):
        _ = model.generate(**inputs, max_new_tokens=150, num_beams=5)
    print("é¢„çƒ­å®Œæˆã€‚")

    # --- 4. æ­£å¼è®¡æ—¶ ---
    print(f"æ­£åœ¨è¿›è¡Œ {TIMING_RUNS} æ¬¡è®¡æ—¶ç”Ÿæˆ...")
    torch.cuda.synchronize() # ç­‰å¾…æ‰€æœ‰GPUæ“ä½œå®Œæˆ
    start_time = time.time()
    
    for _ in range(TIMING_RUNS):
        _ = model.generate(**inputs, max_new_tokens=150, num_beams=5)
        
    torch.cuda.synchronize() # å†æ¬¡ç­‰å¾…ï¼Œç¡®ä¿æ‰€æœ‰ç”Ÿæˆéƒ½å·²ç»“æŸ
    end_time = time.time()
    
    # --- 5. è®¡ç®—å¹¶æŠ¥å‘Šç»“æœ ---
    total_time = end_time - start_time
    avg_latency_ms = (total_time / TIMING_RUNS) * 1000
    
    # è®°å½•å³°å€¼æ˜¾å­˜
    peak_memory_gb = torch.cuda.max_memory_allocated(DEVICE) / 1024**3
    
    print("\n--- æ•ˆç‡æµ‹è¯•ç»“æœ ---")
    print(f"æ¨¡å‹: {model_name}")
    print(f"å¹³å‡å»¶è¿Ÿ (Avg. Latency): {avg_latency_ms:.2f} ms/explanation")
    print(f"å³°å€¼æ˜¾å­˜å ç”¨ (Peak GPU Memory): {peak_memory_gb:.2f} GB")
    
    # æ¸…ç†æ˜¾å­˜ï¼Œä¸ºä¸‹ä¸€ä¸ªæ¨¡å‹åšå‡†å¤‡
    del model
    del tokenizer
    torch.cuda.empty_cache()
    
    return avg_latency_ms, peak_memory_gb

# ==============================================================================
# 3. ä¸»ç¨‹åº
# ==============================================================================

if __name__ == "__main__":
    print("--- å¯åŠ¨æ¨¡å‹æ•ˆç‡æµ‹è¯•è„šæœ¬ ---")
    results = {}

    # --- æµ‹è¯•ä½ çš„æ¨¡å‹ (GenRec-E) ---
    try:
        tokenizer_ours = AutoTokenizer.from_pretrained(YOUR_MODEL_BASE_PATH, local_files_only=True)
        model_ours = AutoModelForSeq2SeqLM.from_pretrained(YOUR_MODEL_BASE_PATH, local_files_only=True)
        model_ours.load_state_dict(torch.load(YOUR_MODEL_WEIGHTS_PATH, map_location=DEVICE), strict=False)
        model_ours = model_ours.to(DEVICE)
        model_ours.eval()
        results['GenRec-E'] = benchmark_model("GenRec-E (Ours)", model_ours, tokenizer_ours)
    except Exception as e:
        print(f"!!! æµ‹è¯•ä½ çš„æ¨¡å‹æ—¶å¤±è´¥: {e}")

    # --- æµ‹è¯•åŸºçº¿æ¨¡å‹ (FLAN-T5-XXL) ---
    try:
        tokenizer_baseline = T5Tokenizer.from_pretrained(BASELINE_MODEL_PATH, local_files_only=True)
        model_baseline = T5ForConditionalGeneration.from_pretrained(
            BASELINE_MODEL_PATH,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            local_files_only=True
        )
        model_baseline.eval()
        results['FLAN-T5-XXL'] = benchmark_model("FLAN-T5-XXL (Baseline)", model_baseline, tokenizer_baseline)
    except Exception as e:
        print(f"!!! æµ‹è¯•åŸºçº¿æ¨¡å‹æ—¶å¤±è´¥: {e}")

    # --- æ‰“å°æœ€ç»ˆçš„å¯¹æ¯”è¡¨æ ¼ ---
    print("\n\n--- æœ€ç»ˆæ•ˆç‡å¯¹æ¯”æŠ¥å‘Š ---")
    print(f"{'Model':<25} | {'Params':<10} | {'Avg. Latency (ms)':<20} | {'Peak GPU Memory (GB)':<20}")
    print("-" * 85)
    
    if 'GenRec-E' in results:
        lat, mem = results['GenRec-E']
        print(f"{'GenRec-E (Ours)':<25} | {'140M':<10} | {lat:<20.2f} | {mem:<20.2f}")
    
    if 'FLAN-T5-XXL' in results:
        lat, mem = results['FLAN-T5-XXL']
        print(f"{'FLAN-T5-XXL (Baseline)':<25} | {'11B':<10} | {lat:<20.2f} | {mem:<20.2f}")

    print("\nğŸ‰ æ•ˆç‡æµ‹è¯•å®Œæˆï¼")