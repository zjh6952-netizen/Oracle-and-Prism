import os
import sys
from argparse import Namespace

import evaluate
import pandas as pd
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import AutoTokenizer


# ==============================================================================
# 1. é…ç½®éƒ¨åˆ† (CONFIGURATION)
# ==============================================================================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")
PROJECT_ROOT = "/root/autodl-tmp/GenRec_Explainer_Project"

# !! å…³é”® !!: è¯·å°† 'YYYYMMDD_HHMMSS' æ›¿æ¢ä¸ºä½ çœŸå®çš„è®­ç»ƒè¾“å‡ºæ–‡ä»¶å¤¹çš„æ—¶é—´æˆ³
YOUR_MODEL_WEIGHTS_PATH = os.path.join(PROJECT_ROOT, "results", "20250918_165141", "best_model.mdl")
YOUR_MODEL_BASE_PATH = os.path.join(PROJECT_ROOT, "models", "bart-base", "facebook", "bart-base")

METRICS_DIR = os.path.join(PROJECT_ROOT, "offline_metrics")
ROUGE_SCRIPT_PATH = os.path.join(METRICS_DIR, "rouge")
BERTSCORE_SCRIPT_PATH = os.path.join(METRICS_DIR, "bertscore")

TEST_DATA_PATH = os.path.join(PROJECT_ROOT, "data", "processed", "explanation_dataset_test.csv")
RESULTS_PATH = os.path.join(PROJECT_ROOT, "results", "evaluation_results_our_model.csv")
HUMAN_EVAL_PATH = os.path.join(PROJECT_ROOT, "results", "human_evaluation_data_for_our_model.csv")

MAX_LENGTH = 768
MAX_OUTPUT_LENGTH = 150
EVAL_BATCH_SIZE = 16
EVAL_NUM_BEAMS = 5
EVAL_REPETITION_PENALTY = 1.2
EVAL_NO_REPEAT_NGRAM_SIZE = 3
EVAL_LENGTH_PENALTY = 1.0
EVAL_MIN_NEW_TOKENS = 0
LOCAL_FILES_ONLY = True


def _prepare_imports():
    """Import project modules from GenRec, preserving your current path layout."""
    genrec_root = os.path.join(PROJECT_ROOT, "GenRec")
    if genrec_root not in sys.path:
        sys.path.insert(0, genrec_root)
    from genrec.data import Dataset  # pylint: disable=import-outside-toplevel
    from genrec.model import GenerativeModel  # pylint: disable=import-outside-toplevel
    from genrec.utils import move_to_cuda  # pylint: disable=import-outside-toplevel
    return Dataset, GenerativeModel, move_to_cuda


def load_your_bart_model(base_path, weights_path):
    """
    åŠ è½½è‡ªå®šä¹‰ GenerativeModelï¼ˆå†…éƒ¨æ˜¯ä½ æ”¹é€ è¿‡çš„ BartForConditionalGenerationï¼‰ã€‚
    strict=True ä¿è¯æƒé‡ç»“æ„å¿…é¡»åŒ¹é…ï¼Œé¿å… silently ignoreã€‚
    """
    print("--- æ­£åœ¨åŠ è½½ä½ å¾®è°ƒå¥½çš„è‡ªå®šä¹‰BARTæ¨¡å‹ ---")
    print(f"  - åŸºç¡€ç»“æ„: {base_path}")
    print(f"  - å¾®è°ƒæƒé‡: {weights_path}")
    try:
        Dataset, GenerativeModel, move_to_cuda = _prepare_imports()
        tokenizer = AutoTokenizer.from_pretrained(base_path, local_files_only=LOCAL_FILES_ONLY, add_prefix_space=True)
        tokenizer.add_tokens(["<mask>"])
        config = Namespace(
            model_name=base_path,
            local_files_only=LOCAL_FILES_ONLY,
            label_smoothing=0.1,
        )
        model = GenerativeModel(config, tokenizer)
        state_dict = torch.load(weights_path, map_location=DEVICE)
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=True)
        if missing_keys or unexpected_keys:
            raise RuntimeError(f"æƒé‡ä¸åŒ¹é…: missing={missing_keys}, unexpected={unexpected_keys}")
        model = model.to(DEVICE)
        model.eval()
        print("ä½ çš„æ¨¡å‹(GenRec-E, è‡ªå®šä¹‰ç»“æ„)åŠ è½½æˆåŠŸã€‚")
        return model, tokenizer, Dataset, move_to_cuda
    except Exception as e:
        print("!!! åŠ è½½ä½ çš„æ¨¡å‹å¤±è´¥! è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ä¸”è®­ç»ƒå·²å®Œæˆã€‚")
        print(f"é”™è¯¯: {e}")
        return None, None, None, None


def main():
    your_model, your_tokenizer, Dataset, move_to_cuda = load_your_bart_model(
        YOUR_MODEL_BASE_PATH, YOUR_MODEL_WEIGHTS_PATH
    )
    if not your_model:
        return

    print("\n--- æ­£åœ¨ä»æœ¬åœ°åŠ è½½è¯„ä¼°æŒ‡æ ‡ ---")
    try:
        rouge = evaluate.load(ROUGE_SCRIPT_PATH)
        print("âœ“ ROUGEè¯„ä¼°æŒ‡æ ‡åŠ è½½æˆåŠŸã€‚")
    except Exception as e:
        print(f"!!! åŠ è½½ROUGEå¤±è´¥: {e}")
        return

    try:
        bertscore = evaluate.load(BERTSCORE_SCRIPT_PATH)
        print("âœ“ BERTScoreè¯„ä¼°æŒ‡æ ‡åŠ è½½æˆåŠŸã€‚")
    except Exception as e:
        print(f"!!! åŠ è½½BERTScoreå¤±è´¥: {e}")
        print("å°†ä»…ä½¿ç”¨ROUGEè¿›è¡Œè¯„ä¼°")
        bertscore = None

    print("\n--- æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®å¹¶æ„å»ºè¯„ä¼°é›† ---")
    if not os.path.exists(TEST_DATA_PATH):
        print(f"!!! é”™è¯¯: æµ‹è¯•é›†æ–‡ä»¶æœªæ‰¾åˆ°! '{TEST_DATA_PATH}'")
        return

    test_set = Dataset(
        your_tokenizer,
        max_length=MAX_LENGTH,
        path=TEST_DATA_PATH,
        max_output_length=MAX_OUTPUT_LENGTH,
        filter_pseudo_labels=False,
    )
    if len(test_set) == 0:
        print("!!! æµ‹è¯•é›†ä¸ºç©ºï¼Œæ— æ³•è¯„ä¼°ã€‚")
        return
    test_loader = DataLoader(
        test_set,
        batch_size=EVAL_BATCH_SIZE,
        shuffle=False,
        collate_fn=test_set.collate_fn,
    )
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_set)}")

    results = []
    print(f"\n--- æ­£åœ¨ä¸ºä½ çš„æ¨¡å‹ (GenRec-E) ç”Ÿæˆ {len(test_set)} æ¡è§£é‡Š ---")
    with torch.no_grad():
        for batch in tqdm(test_loader, total=len(test_loader)):
            gpu_batch = move_to_cuda(batch) if DEVICE == "cuda" else batch
            predictions = your_model.predict(
                gpu_batch,
                num_beams=EVAL_NUM_BEAMS,
                max_length=MAX_OUTPUT_LENGTH,
                repetition_penalty=EVAL_REPETITION_PENALTY,
                no_repeat_ngram_size=EVAL_NO_REPEAT_NGRAM_SIZE,
                length_penalty=EVAL_LENGTH_PENALTY,
                min_new_tokens=EVAL_MIN_NEW_TOKENS,
            )
            for src, refs, pred in zip(batch.input_text, batch.target_text, predictions):
                ref = refs[0] if refs else ""
                item = ""
                if "Recommended Item:" in src:
                    item = src.split("Recommended Item:", 1)[1].split("\n", 1)[0].strip()
                results.append(
                    {
                        "history": src,
                        "item": item,
                        "golden": ref,
                        "prediction": pred,
                    }
                )

    results_df = pd.DataFrame(results)
    references = results_df["golden"].tolist()
    predictions = results_df["prediction"].tolist()

    print("\n--- æ­£åœ¨è®¡ç®—è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡ ---")
    print("æ­£åœ¨è®¡ç®—ROUGE...")
    rouge_scores = rouge.compute(predictions=predictions, references=references)

    bert_scores = None
    if bertscore:
        print("æ­£åœ¨è®¡ç®—BERTScore (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…)...")
        try:
            bert_scores = bertscore.compute(
                predictions=predictions,
                references=references,
                lang="en",
                device=DEVICE,
            )
            print("âœ“ BERTScoreè®¡ç®—å®Œæˆ")
        except Exception as e:
            print(f"!!! BERTScoreè®¡ç®—å¤±è´¥: {e}")
            bert_scores = None

    print("\n--- ä½ çš„æ¨¡å‹ (GenRec-E) è¯„ä¼°ç»“æœ ---")
    print(f"{'Metric':<15} | {'Score':<10}")
    print("-" * 30)
    print(f"{'ROUGE-1':<15} | {rouge_scores.get('rouge1', 0.0):.4f}")
    print(f"{'ROUGE-2':<15} | {rouge_scores.get('rouge2', 0.0):.4f}")
    print(f"{'ROUGE-L':<15} | {rouge_scores.get('rougeL', 0.0):.4f}")
    if bert_scores:
        bert_f1 = sum(bert_scores["f1"]) / len(bert_scores["f1"])
        print(f"{'BERTScore-F1':<15} | {bert_f1:.4f}")

    results_df.to_csv(RESULTS_PATH, index=False)
    results_df.to_csv(HUMAN_EVAL_PATH, index=False)
    print(f"\nè¯¦ç»†ç”Ÿæˆç»“æœå·²ä¿å­˜åˆ°: {RESULTS_PATH}")
    print("\nğŸ‰ ä½ çš„æ¨¡å‹è¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()
