import os
import pandas as pd
import torch
import evaluate 
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from bert_score import BERTScorer

# ==============================================================================
# ==============================================================================

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
PROJECT_ROOT = "/root/autodl-tmp/GenRec_Explainer_Project"

YOUR_MODEL_WEIGHTS_PATH = os.path.join(PROJECT_ROOT, "results", "20250905_163846", "best_model.mdl") 

YOUR_MODEL_BASE_PATH = os.path.join(PROJECT_ROOT, "models", "bart-base", "facebook", "bart-base")

ROBERTA_LOCAL_PATH = os.path.join(PROJECT_ROOT, "models", "roberta-large")

METRICS_DIR = os.path.join(PROJECT_ROOT, "offline_metrics")
ROUGE_SCRIPT_PATH = os.path.join(METRICS_DIR, "rouge")

TEST_DATA_PATH = os.path.join(PROJECT_ROOT, "data", "processed", "explanation_dataset_test.csv")
RESULTS_PATH = os.path.join(PROJECT_ROOT, "results", "evaluation_results_our_model.csv")
HUMAN_EVAL_PATH = os.path.join(PROJECT_ROOT, "results", "human_evaluation_data_for_our_model.csv")

# ==============================================================================
# ==============================================================================

def load_your_bart_model(base_path, weights_path):
    """Load the finetuned BART student model."""
    print(f"--- Loading your finetuned BART model ---")
    print(f"  - Base architecture: {base_path}")
    print(f"  - Finetuned weights: {weights_path}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(base_path, local_files_only=True)
        model = AutoModelForSeq2SeqLM.from_pretrained(base_path, local_files_only=True)
        model.load_state_dict(torch.load(weights_path, map_location=DEVICE), strict=False)
        model = model.to(DEVICE)
        model.eval()
        print("Your model loaded successfully.")
        return model, tokenizer
    except Exception as e:
        print(f"!!! Failed to load your model. Ensure paths are correct and training is completed."); print(f"Error: {e}"); return None, None

def generate_explanation(model, tokenizer, history, item):
    """Generate one explanation for a history-item pair."""
    prompt = f"Instruction: Generate a personalized explanation for the given recommendation.\nInput: User History: {history}\nRecommended Item: {item}"
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=150, num_beams=5)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# ==============================================================================
# ==============================================================================

def main():
    your_model, your_tokenizer = load_your_bart_model(YOUR_MODEL_BASE_PATH, YOUR_MODEL_WEIGHTS_PATH)
    if not your_model: 
        return
        
    print("\n--- Loading evaluation metrics from local files ---")
    try:
        rouge = evaluate.load(ROUGE_SCRIPT_PATH)
        print("ROUGE metric loaded successfully.")
    except Exception as e:
        print(f"!!! Failed to load ROUGE: {e}"); return

    print("Initializing BERTScorer (loading roberta-large from local path)...")
    try:
        scorer = BERTScorer(model_type=ROBERTA_LOCAL_PATH, lang="en", rescale_with_baseline=True, device=DEVICE)
        print("BERTScorer initialized successfully.")
    except Exception as e:
        print(f"!!! Failed to initialize BERTScorer. Check whether roberta-large files are complete: {e}"); return
    
    print(f"\n--- Loading test data ---")
    try:
        df = pd.read_csv(TEST_DATA_PATH)
        df = df.head(10)
    except FileNotFoundError:
        print(f"!!! Error: test dataset not found: '{TEST_DATA_PATH}'"); return

    results = []
    print(f"\n--- Generating {len(df)} explanations with your model ---")
    for index, row in tqdm(df.iterrows(), total=len(df)):
        history, item, reference = str(row['history']), str(row['recommended_item']), str(row['explanation'])
        your_pred = generate_explanation(your_model, your_tokenizer, history, item)
        results.append({ 
            'history': history, 'item': item,
            'golden': reference, 'prediction': your_pred
        })
        
    results_df = pd.DataFrame(results)
    references = results_df['golden'].tolist()
    predictions = results_df['prediction'].tolist()
    
    print("\n--- Computing automatic evaluation metrics ---")
    print("Computing ROUGE...")
    rouge_scores = rouge.compute(predictions=predictions, references=references)
    
    print("Computing BERTScore...")
    (P, R, F1) = scorer.score(predictions, references, verbose=False)
    bert_f1 = F1.mean().item()

    print("\n--- Evaluation Results: Your Model ---")
    print(f"{'Metric':<15} | {'Score':<10}")
    print("-" * 30)
    print(f"{'ROUGE-1':<15} | {rouge_scores.get('rouge1', 0.0):.4f}")
    print(f"{'ROUGE-2':<15} | {rouge_scores.get('rouge2', 0.0):.4f}")
    print(f"{'ROUGE-L':<15} | {rouge_scores.get('rougeL', 0.0):.4f}")
    print(f"{'BERTScore-F1':<15} | {bert_f1:.4f}")
    
    results_df.to_csv(RESULTS_PATH, index=False)
    results_df.to_csv(HUMAN_EVAL_PATH, index=False)
    print(f"\nDetailed generation results saved to: {RESULTS_PATH}")
    print("\nYour model evaluation is complete.")

if __name__ == "__main__":
    main()
