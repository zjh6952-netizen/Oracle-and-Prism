import os
import time
import pandas as pd
from tqdm import tqdm
from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
import csv # ä½¿ç”¨æ›´åº•å±‚çš„csvåº“æ¥å®ç°å†…å­˜é«˜æ•ˆçš„è¿½åŠ å†™å…¥
import re
import json

# ==============================================================================
# 1. é…ç½®éƒ¨åˆ† (CONFIGURATION)
# ==============================================================================

PROJECT_ROOT = "/root/autodl-tmp/GenRec_Explainer_Project"
TEACHER_MODEL_PATH = os.path.join(PROJECT_ROOT, "models", "flan-t5-xxl")
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "data", "processed")
os.makedirs(OUTPUT_DIR, exist_ok=True)

INPUT_FILES = {
    "train": os.path.join(PROJECT_ROOT, "data", "raw", "yelp_sequences_train.csv"),
    "test": os.path.join(PROJECT_ROOT, "data", "raw", "yelp_sequences_test.csv")
}
MAX_INPUT_LENGTH = 512
MAX_NEW_TOKENS = 150

# ç”Ÿæˆå¤šä¸ªå€™é€‰ååšé‡æ’ï¼Œæ˜¾è‘—å‡å°‘æ•™å¸ˆâ€œèƒ¡è¯´â€å’Œâ€œå¤è¯»â€
NUM_CANDIDATES = 4
SAMPLING_TOP_P = 0.9
SAMPLING_TEMPERATURE = 0.8
REPETITION_PENALTY = 1.2
NO_REPEAT_NGRAM_SIZE = 3

# ä¼ªæ ‡ç­¾è´¨é‡è¿‡æ»¤é˜ˆå€¼
MIN_EXPLANATION_TOKENS = 6
MAX_EXPLANATION_TOKENS = 80
MAX_REPEAT_RATIO = 0.55
MIN_SOURCE_OVERLAP = 0.12
MIN_QUALITY_SCORE = 0.35
PROGRESS_SAVE_EVERY = 100

# è¿™æ˜¯ä½ åº”è¯¥åœ¨â€œYelpç‰ˆæœ¬â€çš„æ•°æ®ç”Ÿæˆè„šæœ¬ä¸­ä½¿ç”¨çš„æœ€ç»ˆPrompt

EXPLANATION_PROMPT_TEMPLATE = """Generate a short and faithful explanation for the following local business recommendation.
The explanation MUST be based ONLY on the user's visit history. Do NOT invent reasons.

Context:
- User Visit History: {history}
- Recommended Business: {item_to_explain}

Explanation:
"""

# ==============================================================================
# 2. æ¨¡å‹åŠ è½½ä¸æ¨ç†å‡½æ•° (MODEL & INFERENCE) - RTX 4090 ä¼˜åŒ–
# ==============================================================================

print("--- åˆå§‹åŒ–æ•™å¸ˆæ¨¡å‹ (RTX 4090 - BF16 é«˜æ€§èƒ½æ¨¡å¼) ---")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ä½¿ç”¨çš„è®¾å¤‡: {DEVICE}")

try:
    tokenizer = T5Tokenizer.from_pretrained(TEACHER_MODEL_PATH)
    model = T5ForConditionalGeneration.from_pretrained(
        TEACHER_MODEL_PATH,
        torch_dtype=torch.bfloat16,
        device_map={"": 0}
    )
    print("æ•™å¸ˆæ¨¡å‹ä»¥BF16é«˜æ€§èƒ½æ¨¡å¼åŠ è½½æˆåŠŸã€‚")
except Exception as e:
    print(f"!!! åŠ è½½æ•™å¸ˆæ¨¡å‹å¤±è´¥! è¯·æ£€æŸ¥è·¯å¾„ '{TEACHER_MODEL_PATH}' æ˜¯å¦æ­£ç¡®ä¸”å®Œæ•´ã€‚")
    print(f"é”™è¯¯ä¿¡æ¯: {e}")
    exit()

def decode_history(raw_text):
    """Convert encoded format (\\i\\sep, \\sep, \\i, \\n) into readable text."""
    text = str(raw_text)
    if not text or text.lower() == "nan":
        return ""
    text = text.replace("\\i\\sep", " ")
    text = text.replace("\\sep", ", ")
    text = re.sub(r"\\i\s*[\d,\s]*", " ", text)
    text = text.replace("\\n", " ")
    text = re.sub(r"\s+", " ", text).strip()
    return text


def tokenize_words(text):
    return re.findall(r"[a-zA-Z0-9]+", str(text).lower())


def compute_quality(explanation, history, target):
    """
    è´¨é‡åˆ† = æ¥æºé‡åˆåº¦ + å¤šæ ·æ€§ - å¤è¯»æƒ©ç½š
    ç›®çš„ï¼šè¿‡æ»¤å¹»è§‰/æ¨¡æ¿åŒ–é‡å¤è§£é‡Šã€‚
    """
    tokens = tokenize_words(explanation)
    if not tokens:
        return -1.0, 0.0, 1.0, 0

    src_tokens = set(tokenize_words(history) + tokenize_words(target))
    overlap_hits = sum(1 for tok in tokens if tok in src_tokens)
    overlap_ratio = overlap_hits / len(tokens)

    unique_ratio = len(set(tokens)) / len(tokens)
    repeat_ratio = 1.0 - unique_ratio

    score = overlap_ratio * 1.8 + unique_ratio * 0.8 - repeat_ratio * 1.2
    return score, overlap_ratio, repeat_ratio, len(tokens)


def is_good_explanation(score, overlap_ratio, repeat_ratio, token_len):
    if token_len < MIN_EXPLANATION_TOKENS or token_len > MAX_EXPLANATION_TOKENS:
        return False
    if overlap_ratio < MIN_SOURCE_OVERLAP:
        return False
    if repeat_ratio > MAX_REPEAT_RATIO:
        return False
    if score < MIN_QUALITY_SCORE:
        return False
    return True


def generate_explanation(prompt, history, target):
    """ä½¿ç”¨å¤šå€™é€‰é‡æ’ç”Ÿæˆè§£é‡Šï¼Œé™ä½å¹»è§‰å’Œå¤è¯»ã€‚"""
    try:
        inputs = tokenizer(
            prompt, 
            return_tensors="pt",
            max_length=MAX_INPUT_LENGTH,
            truncation=True,
        ).to(DEVICE)

        sampled_outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            top_p=SAMPLING_TOP_P,
            temperature=SAMPLING_TEMPERATURE,
            num_return_sequences=NUM_CANDIDATES,
            repetition_penalty=REPETITION_PENALTY,
            no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,
            eos_token_id=tokenizer.eos_token_id,
        )
        sampled_candidates = tokenizer.batch_decode(sampled_outputs, skip_special_tokens=True)

        best_text = ""
        best_metrics = (-1.0, 0.0, 1.0, 0)
        for cand in sampled_candidates:
            metrics = compute_quality(cand, history, target)
            if metrics[0] > best_metrics[0]:
                best_text = cand
                best_metrics = metrics

        # è‹¥é‡‡æ ·å€™é€‰è´¨é‡å¤ªå·®ï¼Œç”¨ beam-search å†å…œåº•ä¸€æ¬¡ã€‚
        if not is_good_explanation(*best_metrics):
            beam_outputs = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=False,
                num_beams=5,
                repetition_penalty=REPETITION_PENALTY,
                no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,
                eos_token_id=tokenizer.eos_token_id,
            )
            beam_text = tokenizer.decode(beam_outputs[0], skip_special_tokens=True)
            beam_metrics = compute_quality(beam_text, history, target)
            if beam_metrics[0] > best_metrics[0]:
                best_text = beam_text
                best_metrics = beam_metrics

        return best_text, best_metrics
    except Exception as e:
        print(f"\n!!! æ¨¡å‹æ¨ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return "Error: Generation failed.", (-1.0, 0.0, 1.0, 0)

# ==============================================================================
# 3. ä¸»ç¨‹åº (MAIN LOGIC) - å†…å­˜ä¼˜åŒ–ç‰ˆ
# ==============================================================================

def main():
    global_kept = 0
    global_skipped = 0

    # å¾ªç¯å¤„ç†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    for split, filepath in INPUT_FILES.items():
        print(f"\n--- å¼€å§‹åˆ›å»ºè§£é‡Šæ•°æ®é›†: {split} set (å†…å­˜ä¼˜åŒ–æ¨¡å¼) ---")
        output_path = os.path.join(OUTPUT_DIR, f"explanation_dataset_{split}.csv")
        progress_path = output_path + ".progress.json"
        split_kept = 0
        split_skipped = 0
        
        try:
            df = pd.read_csv(filepath)
            print(f"æˆåŠŸè¯»å– {len(df)} æ¡åŸå§‹æ•°æ®ã€‚")
        except FileNotFoundError:
            print(f"!!! é”™è¯¯: åŸå§‹æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°! è¯·æ£€æŸ¥è·¯å¾„ '{filepath}'")
            continue

        start_index = 0
        if os.path.exists(progress_path):
            try:
                with open(progress_path, "r", encoding="utf-8") as pf:
                    progress_state = json.load(pf)
                start_index = int(progress_state.get("next_raw_index", 0))
                print(f"å‘ç°è¿›åº¦æ–‡ä»¶ï¼Œå°†ä»åŸå§‹è¡Œå· {start_index} ç»§ç»­å¤„ç†ã€‚")
            except Exception:
                start_index = 0
                print("è¿›åº¦æ–‡ä»¶æŸåï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†ã€‚")
        elif os.path.exists(output_path):
            # è‹¥è¿›åº¦æ–‡ä»¶ç¼ºå¤±ï¼Œä¼˜å…ˆä» raw_index æ¢å¤ï¼›æ²¡æœ‰è¯¥åˆ—æ—¶åªèƒ½ä»å¤´å¼€å§‹ç¡®ä¿æ­£ç¡®æ€§ã€‚
            try:
                existing_df = pd.read_csv(output_path, usecols=["raw_index"])
                if len(existing_df) > 0:
                    start_index = int(existing_df["raw_index"].max()) + 1
                    print(f"è¿›åº¦æ–‡ä»¶ç¼ºå¤±ï¼Œå·²ä» raw_index æ¢å¤åˆ° {start_index}ã€‚")
            except Exception:
                start_index = 0
                print("è¿›åº¦æ–‡ä»¶ç¼ºå¤±ä¸”è¾“å‡ºæ–‡ä»¶ä¸å« raw_indexï¼Œå°†ä»å¤´é‡æ–°ç”Ÿæˆä»¥é¿å…é”™ä½ã€‚")

        if start_index >= len(df):
            print("è¯¥ split å·²å…¨éƒ¨å¤„ç†å®Œæˆï¼Œè·³è¿‡ã€‚")
            continue
        
        open_mode = "a" if start_index > 0 else "w"
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨æµå¼å†™å…¥ + å¯æ¢å¤è¿›åº¦ ---
        with open(output_path, open_mode, newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # å¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼ˆæˆ–ç©ºæ–‡ä»¶ï¼‰ï¼Œå°±å…ˆå†™å…¥è¡¨å¤´
            if start_index == 0:
                writer.writerow([
                    "raw_index",
                    "user_id",
                    "history",
                    "recommended_item",
                    "explanation",
                    "quality_score",
                    "source_overlap",
                    "repeat_ratio",
                    "explanation_tokens"
                ])

            # å¾ªç¯å¤„ç†å‰©ä½™çš„æ•°æ®
            for raw_idx in tqdm(range(start_index, len(df)), initial=start_index, total=len(df), desc=f"Generating for {split}"):
                row = df.iloc[raw_idx]
                history = decode_history(str(row["history"]))
                target = decode_history(str(row["target"]))
                user_id = row.get('user_id', 1)
                
                prompt_text = EXPLANATION_PROMPT_TEMPLATE.format(history=history, item_to_explain=target)
                explanation_text, metrics = generate_explanation(prompt_text, history, target)
                score, overlap_ratio, repeat_ratio, token_len = metrics

                should_write = True
                if "Error:" in explanation_text or explanation_text.strip() == "":
                    should_write = False
                if should_write and not is_good_explanation(score, overlap_ratio, repeat_ratio, token_len):
                    should_write = False

                if should_write:
                    writer.writerow([
                        raw_idx,
                        user_id,
                        history,
                        target,
                        explanation_text,
                        round(score, 4),
                        round(overlap_ratio, 4),
                        round(repeat_ratio, 4),
                        token_len
                    ])
                    split_kept += 1
                else:
                    split_skipped += 1

                if raw_idx % PROGRESS_SAVE_EVERY == 0:
                    with open(progress_path, "w", encoding="utf-8") as pf:
                        json.dump({"next_raw_index": raw_idx + 1}, pf)

            # ç¡®ä¿æœ¬ split ç»“æŸåæŒä¹…åŒ–åˆ°æœ«å°¾
            with open(progress_path, "w", encoding="utf-8") as pf:
                json.dump({"next_raw_index": len(df)}, pf)

        print(f"\nğŸ‰ {split} set å¤„ç†å®Œæ¯•ï¼æœ€ç»ˆæ•°æ®é›†å·²å®Œæ•´ä¿å­˜åˆ°: {output_path}")
        print(f"ä¿ç•™æ ·æœ¬: {split_kept} | è¿‡æ»¤æ ·æœ¬: {split_skipped}")
        global_kept += split_kept
        global_skipped += split_skipped

    print("\n=== å…¨éƒ¨æ•°æ®ç”Ÿæˆå®Œæˆ ===")
    print(f"æ€»ä¿ç•™æ ·æœ¬: {global_kept}")
    print(f"æ€»è¿‡æ»¤æ ·æœ¬: {global_skipped}")

if __name__ == "__main__":
    main()
