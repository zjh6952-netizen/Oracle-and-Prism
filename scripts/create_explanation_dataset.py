import os
import time
import pandas as pd
from tqdm import tqdm
from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
import csv # ä½¿ç”¨æ›´åº•å±‚çš„csvåº“æ¥å®ç°å†…å­˜é«˜æ•ˆçš„è¿½åŠ å†™å…¥

# ==============================================================================
# 1. é…ç½®éƒ¨åˆ† (CONFIGURATION)
# ==============================================================================

PROJECT_ROOT = "/root/autodl-tmp/GenRec_Explainer_Project"
TEACHER_MODEL_PATH = os.path.join(PROJECT_ROOT, "models", "flan-t5-xxl")
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "data", "processed")
os.makedirs(OUTPUT_DIR, exist_ok=True)

INPUT_FILES = {
    "train": os.path.join(PROJECT_ROOT, "data", "raw", "yelp_sequences_train.csv"),
    "test": os.path.join(PROJECT_ROOT, "data", "raw", "yelp_sequences_test.csv")
}
MAX_INPUT_LENGTH = 512

# è¿™æ˜¯ä½ åº”è¯¥åœ¨â€œYelpç‰ˆæœ¬â€çš„æ•°æ®ç”Ÿæˆè„šæœ¬ä¸­ä½¿ç”¨çš„æœ€ç»ˆPrompt

EXPLANATION_PROMPT_TEMPLATE = """Generate a short and faithful explanation for the following local business recommendation.
The explanation MUST be based ONLY on the user's visit history. Do NOT invent reasons.

Context:
- User Visit History: {history}
- Recommended Business: {item_to_explain}

Explanation:
"""

# ==============================================================================
# 2. æ¨¡å‹åŠ è½½ä¸æ¨ç†å‡½æ•° (MODEL & INFERENCE) - RTX 4090 ä¼˜åŒ–
# ==============================================================================

print("--- åˆå§‹åŒ–æ•™å¸ˆæ¨¡å‹ (RTX 4090 - BF16 é«˜æ€§èƒ½æ¨¡å¼) ---")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ä½¿ç”¨çš„è®¾å¤‡: {DEVICE}")

try:
    tokenizer = T5Tokenizer.from_pretrained(TEACHER_MODEL_PATH)
    model = T5ForConditionalGeneration.from_pretrained(
        TEACHER_MODEL_PATH,
        torch_dtype=torch.bfloat16,
        device_map={"": 0}
    )
    print("æ•™å¸ˆæ¨¡å‹ä»¥BF16é«˜æ€§èƒ½æ¨¡å¼åŠ è½½æˆåŠŸã€‚")
except Exception as e:
    print(f"!!! åŠ è½½æ•™å¸ˆæ¨¡å‹å¤±è´¥! è¯·æ£€æŸ¥è·¯å¾„ '{TEACHER_MODEL_PATH}' æ˜¯å¦æ­£ç¡®ä¸”å®Œæ•´ã€‚")
    print(f"é”™è¯¯ä¿¡æ¯: {e}")
    exit()

def generate_explanation(prompt):
    """ä½¿ç”¨åŠ è½½å¥½çš„BF16æ¨¡å‹ç”Ÿæˆè§£é‡Šï¼Œå¹¶åŠ å…¥äº†å®‰å…¨æˆªæ–­ã€‚"""
    try:
        inputs = tokenizer(
            prompt, 
            return_tensors="pt",
            max_length=MAX_INPUT_LENGTH,
            truncation=True,
        ).to(DEVICE)
        outputs = model.generate(**inputs, max_new_tokens=150, no_repeat_ngram_size=2)
        return tokenizer.decode(outputs[0], skip_special_tokens=True)
    except Exception as e:
        print(f"\n!!! æ¨¡å‹æ¨ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return "Error: Generation failed."

# ==============================================================================
# 3. ä¸»ç¨‹åº (MAIN LOGIC) - å†…å­˜ä¼˜åŒ–ç‰ˆ
# ==============================================================================

def main():
    # å¾ªç¯å¤„ç†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    for split, filepath in INPUT_FILES.items():
        print(f"\n--- å¼€å§‹åˆ›å»ºè§£é‡Šæ•°æ®é›†: {split} set (å†…å­˜ä¼˜åŒ–æ¨¡å¼) ---")
        output_path = os.path.join(OUTPUT_DIR, f"explanation_dataset_{split}.csv")
        
        try:
            df = pd.read_csv(filepath)
            print(f"æˆåŠŸè¯»å– {len(df)} æ¡åŸå§‹æ•°æ®ã€‚")
        except FileNotFoundError:
            print(f"!!! é”™è¯¯: åŸå§‹æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°! è¯·æ£€æŸ¥è·¯å¾„ '{filepath}'")
            continue

        start_index = 0
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šä¸å†æŠŠæ—§æ–‡ä»¶è¯»å…¥å†…å­˜ï¼Œåªç”¨å®ƒæ¥ç¡®å®šèµ·å§‹ä½ç½® ---
        if os.path.exists(output_path):
            try:
                # åªè¯»å–ä¸€å°éƒ¨åˆ†æ¥è·å–è¡Œæ•°ï¼Œé¿å…åŠ è½½æ•´ä¸ªæ–‡ä»¶
                processed_df_len = pd.read_csv(output_path, usecols=[0]).shape[0]
                start_index = processed_df_len
                print(f"å‘ç°å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å« {start_index} æ¡æ•°æ®ã€‚å°†ä»è¯¥ä½ç½®å¼€å§‹è¿½åŠ ã€‚")
            except (pd.errors.EmptyDataError, FileNotFoundError):
                start_index = 0
                print("å‘ç°ç©ºçš„æˆ–æŸåçš„è¾“å‡ºæ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹åˆ›å»ºã€‚")
        
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨'a' (append)æ¨¡å¼å’Œcsvåº“è¿›è¡Œæµå¼å†™å…¥ ---
        with open(output_path, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # å¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼ˆæˆ–ç©ºæ–‡ä»¶ï¼‰ï¼Œå°±å…ˆå†™å…¥è¡¨å¤´
            if start_index == 0:
                writer.writerow(["user_id", "history", "recommended_item", "explanation"])

            # å¾ªç¯å¤„ç†å‰©ä½™çš„æ•°æ®
            for index, row in tqdm(df.iloc[start_index:].iterrows(), initial=start_index, total=len(df), desc=f"Generating for {split}"):
                history = str(row['history'])
                target = str(row['target'])
                user_id = row.get('user_id', 1)
                
                prompt_text = EXPLANATION_PROMPT_TEMPLATE.format(history=history, item_to_explain=target)
                explanation_text = generate_explanation(prompt_text)
                
                # --- æ ¸å¿ƒä¿®æ”¹ï¼šç”Ÿæˆä¸€æ¡ï¼Œå°±ç«‹åˆ»å†™å…¥ç£ç›˜ ---
                if "Error:" not in explanation_text and explanation_text.strip() != "":
                    writer.writerow([user_id, history, target, explanation_text])

        print(f"\nğŸ‰ {split} set å¤„ç†å®Œæ¯•ï¼æœ€ç»ˆæ•°æ®é›†å·²å®Œæ•´ä¿å­˜åˆ°: {output_path}")

if __name__ == "__main__":
    main()