import os
import time
import pandas as pd
from tqdm import tqdm
from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
import csv

# ==============================================================================
# ==============================================================================

PROJECT_ROOT = "/root/autodl-tmp/GenRec_Explainer_Project"
TEACHER_MODEL_PATH = os.path.join(PROJECT_ROOT, "models", "flan-t5-xxl")
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "data", "processed")
os.makedirs(OUTPUT_DIR, exist_ok=True)

INPUT_FILES = {
    "train": os.path.join(PROJECT_ROOT, "data", "raw", "movielens_sequence_train.csv"),
    "test": os.path.join(PROJECT_ROOT, "data", "raw", "movielens_sequences_test.csv")
}
MAX_INPUT_LENGTH = 512

EXPLANATION_PROMPT_TEMPLATE = """
Generate a short, personalized, and persuasive explanation for the following recommendation.
Context:
- User's movie viewing history: {history}
- Recommended movie: {item_to_explain}
Task: Explain WHY this is a good recommendation based on the user's history.
- Be specific: Link features of the recommended movie (like genre, director, actors, theme) to patterns in the history.
- Be natural: Sound like a genuine recommendation from a friend.
- Be concise: Ideally one or two sentences.
- Start the explanation directly.
Explanation:
"""

# ==============================================================================
# ==============================================================================

print("--- Initializing teacher model (BF16 high-performance mode) ---")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {DEVICE}")

try:
    tokenizer = T5Tokenizer.from_pretrained(TEACHER_MODEL_PATH)
    model = T5ForConditionalGeneration.from_pretrained(
        TEACHER_MODEL_PATH,
        torch_dtype=torch.bfloat16,
        device_map={"": 0}
    )
    print("Teacher model loaded successfully in BF16 mode.")
except Exception as e:
    print(f"!!! Failed to load teacher model. Check whether path '{TEACHER_MODEL_PATH}' is correct and complete.")
    print(f"Error details: {e}")
    exit()

def generate_explanation(prompt):
    """Generate an explanation with the loaded BF16 model and safe truncation."""
    try:
        inputs = tokenizer(
            prompt, 
            return_tensors="pt",
            max_length=MAX_INPUT_LENGTH,
            truncation=True,
        ).to(DEVICE)
        outputs = model.generate(**inputs, max_new_tokens=150, no_repeat_ngram_size=2)
        return tokenizer.decode(outputs[0], skip_special_tokens=True)
    except Exception as e:
        print(f"\n!!! Model inference error: {e}")
        return "Error: Generation failed."

# ==============================================================================
# ==============================================================================

def main():
    for split, filepath in INPUT_FILES.items():
        print(f"\n--- Building explanation dataset: {split} set (memory-optimized mode) ---")
        output_path = os.path.join(OUTPUT_DIR, f"explanation_dataset_{split}.csv")
        
        try:
            df = pd.read_csv(filepath)
            print(f"Loaded {len(df)} raw rows successfully.")
        except FileNotFoundError:
            print(f"!!! Error: raw data file not found. Check path '{filepath}'")
            continue

        start_index = 0
        if os.path.exists(output_path):
            try:
                processed_df_len = pd.read_csv(output_path, usecols=[0]).shape[0]
                start_index = processed_df_len
                print(f"Existing output file found with {start_index} rows. Appending from that point.")
            except (pd.errors.EmptyDataError, FileNotFoundError):
                start_index = 0
                print("Output file is empty or corrupted. Recreating from scratch.")
        
        with open(output_path, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            if start_index == 0:
                writer.writerow(["user_id", "history", "recommended_item", "explanation"])

            for index, row in tqdm(df.iloc[start_index:].iterrows(), initial=start_index, total=len(df), desc=f"Generating for {split}"):
                history = str(row['history'])
                target = str(row['target'])
                user_id = row.get('user_id', 1)
                
                prompt_text = EXPLANATION_PROMPT_TEMPLATE.format(history=history, item_to_explain=target)
                explanation_text = generate_explanation(prompt_text)
                
                if "Error:" not in explanation_text and explanation_text.strip() != "":
                    writer.writerow([user_id, history, target, explanation_text])

        print(f"\n{split} set finished. Final dataset saved to: {output_path}")

if __name__ == "__main__":
    main()
